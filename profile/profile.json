{
  "education": [
    {
      "awards_achievements": [
        "GPA: 3.91/4.3"
      ],
      "degree": "Master of Applied Computer Science",
      "department": "Computer Science",
      "duration": "2021-2022",
      "school_name": "Dalhousie University"
    },
    {
      "awards_achievements": [
        "GPA: 8.44/10.0"
      ],
      "degree": "Bachelor of Information Techonology",
      "department": "Information Techonology",
      "duration": "2015-2019",
      "school_name": "Gujarat Technological University"
    }
  ],
  "github_repo_root_url": "https://github.com/Jarvis-Consulting-Group/jarvis_data_eng-KunjPatel03",
  "highlighted_projects": [
    {
      "description": "Developed a robust RDBMS from scratch using Java, providing support for SQL queries, CRUD operations, and transactions. Implemented a sophisticated parsing and validation mechanism utilizing complex regex expressions for accurate data processing. Designed a custom file format to efficiently store data and metadata. Incorporated multiuser concurrent execution capabilities to ensure data consistency during simultaneous interactions. Utilized reverse engineering techniques to generate an Entity-Relationship Diagram (ERD) for visual representation of the database structure. This project showcased my ability to create advanced software systems, empowering users to effectively manage their data and leverage the power of SQL queries.",
      "git_url": "https://github.com/KunjPatel03/RelationalDatabase",
      "name": "Relational Database Management System"
    },
    {
      "description": "A console-based chess game application was developed in Java, offering an engaging experience for users to play against the computer. The application adhered to SOLID principles and employed the factory design pattern to ensure maintainability and extensibility. A Test-Driven Development (TDD) approach was applied, with extensive JUnit test cases written to validate the code and modularize its components. The development process followed an Agile methodology, allowing for iterative and collaborative development. To streamline the development workflow, a CI/CD pipeline was implemented to automate monitoring and deployment processes. This comprehensive approach resulted in a well-structured and thoroughly tested chess game application, delivering a seamless and enjoyable user experience.",
      "git_url": "https://github.com/KunjPatel03/Chess",
      "name": "Chess game"
    }
  ],
  "jarvis_projects": [
    {
      "description": "A user-friendly solution to efficiently manage a 10-server cluster. The solution allows server users to easily monitor hardware and usage information, ensuring smooth operation and maintenance. Implemented with bash scripts, it seamlessly integrates into existing workflows. PostgreSQL (psql) is chosen as the database technology for reliable data storage. The project is implemented within a Docker container for consistent execution and compatibility. Using Git version control, development progress is tracked. This monitoring solution empowers users to optimize resource allocation, enhancing stability and efficiency of the server cluster for seamless operations.",
      "git_url": "/linux_sql",
      "name": "Cluster Monitor"
    },
    {
      "description": [
        "JDBC App: The JDBC application is a robust system for performing essential CRUD operations (Create, Read, Update, Delete) on a database, utilizing the powerful JDBC API. It integrates a PostgreSQL database within a Docker container environment, with Maven as the package management and dependency resolution tool, and Git as the chosen version control system.",
        "Grep App: Grep is a Java-based application offering two efficient approaches for pattern searching within text files. The Stream-Based Approach utilizes Java Streams and functional programming to process large datasets with minimal memory usage, ideal for handling massive files. The Traditional Approach uses conventional file I/O and looping for moderate-sized files. Comprehensive unit testing with JUnit and Mockito ensures application correctness. Dockerizing the application facilitates easy distribution and deployment."
      ],
      "git_url": "/core_java/",
      "name": "Java Applications"
    },
    {
      "description": "Utilized advanced data analytics techniques to analyze a company's transactional data and gain insights into customer shopping behavior, contributing to the development of targeted marketing strategies. Employing Python as the primary programming language, I harnessed the power of Pandas, Numpy, Matplotlib, and SQLAlchemy libraries to perform comprehensive data analysis. The entire analysis was conducted within a Jupyter notebook environment, where I seamlessly accessed data from both CSV files and a PostgreSQL database, sourced from the company's data warehouse. Operating on a Jupyter server, the notebook generated detailed visualizations, including plots illustrating monthly sales trends, order placements, customer counts, and, significantly, Recency, Frequency, and Monetary segmentation of the customer base. This project showcased my proficiency in data analysis, software tools, and database integration, enabling data-driven insights to drive strategic decision-making.",
      "git_url": "/python_data_analytics",
      "name": "Python Data Analytics"
    },
    {
      "description": "Actively contributed to an ongoing Python data analytics project centered on optimizing core Hadoop components, including HDFS, MapReduce, and YARN, to efficiently process and analyze substantial datasets. This initiative provided exposure to enhancing Hadoop's performance within a distributed ecosystem. Leveraging Apache Hive and Zeppelin Notebook on the Google Cloud Platform, the project addressed real-world business challenges by executing powerful HQL statements for data exploration. Additionally, responsibilities encompassed strategic setup of a master node and two worker nodes, forming a robust Hadoop cluster to efficiently manage datasets exceeding 21 million records. Proficiency was gained in utilizing cutting-edge technologies such as Hadoop, Hive, YARN, and MapReduce, enhancing skills in optimizing data storage, processing, and analysis.",
      "git_url": "/hadoop",
      "name": "Hadoop"
    }
  ],
  "name": "Kunj Patel",
  "others": [
    {
      "bullets": [
        "Anime Enthusiast",
        "Travel and exploration"
      ],
      "title": "Activities/Hobbies"
    }
  ],
  "professional_experience": [
    {
      "company": "Jarvis",
      "description": "Led the development of user-friendly solution for efficient cluster management, utilizing bash scripts, PostgreSQL and Docker. Created efficient and robust databases, ensuring smooth operations and optimizing performance. Collaborated closely with clients to understand their database requirements and provided customized solutions. Crafted robust Java applications, including a comprehensive JDBC system and a versatile Grep application for pattern searching. Leveraged advanced data analytics techniques to glean insights from transactional data, employing Pandas, Numpy, and Matplotlib within a Jupyter notebook environment. Significantly contributed to an ongoing initiative focused on optimizing core Hadoop components, utilizing Apache Hive and Zeppelin Notebook on Google Cloud Platform, and strategically configuring a master node and two worker nodes for streamlined data processing.",
      "duration": "2023-present",
      "title": "Data Engineer"
    },
    {
      "company": "Wonolo",
      "description": "Designed and implemented a customer lifecycle tracking data asset, improving end-user accessibility and enabling faster insights. Optimized DBT incremental models for improved query performance and data accuracy. Implemented automated tests throughout the pipeline, significantly enhancing data quality and reducing error rates. Optimized SQL queries to improve query performance and ETL processing time. Collaborated with cross-functional teams to gather requirements and design data solutions that meet business needs. Coauthored an enterprise-level Data Warehouse Architecture Narrative. Designed and created data models using ERD diagrams to ensure efficient data storage and retrieval. Communicated effectively with team members and stakeholders to ensure alignment and transparency in project goals and timelines. Demonstrated strong problem-solving skills to troubleshoot and resolve technical issues.",
      "duration": "2022-2023",
      "title": "Data Engineer"
    },
    {
      "company": "Exxat Systems",
      "description": "Generated client-specific reports through the development of SQL scripts, resulting in a 40% reduction in report generation time and enhancing overall efficiency. Analyzed software issues to identify root causes and implemented technical resolutions, resulting in a 13% performance improvement. Conducted thorough product testing, promptly communicating faults to the QA team and devising effective workarounds for customers. Implemented cost-effective technology solutions by assessing requirements and ensuring clear communication with stakeholders. Contributed to streamlined processes, optimized performance, and improved customer satisfaction through proficient report generation, problem-solving, and effective communication.",
      "duration": "2019-2021",
      "title": "Jr. Product Support Analyst"
    }
  ],
  "skills": {
    "competent": [
      "PySpark",
      "Pandas",
      "JavaScript",
      "React",
      "Node",
      "Snowflake",
      "Amazon Web Services",
      "CI/CD"
    ],
    "familiar": [
      "Spark",
      "Kubernetes",
      "Hadoop",
      "Databricks",
      "Google Cloud Platform"
    ],
    "proficient": [
      "Java",
      "Python",
      "SQL",
      "Linux/Bash",
      "Agile/Scrum",
      "Git",
      "Docker"
    ]
  },
  "summary": "With over two years of experience as a results-driven Data Engineer and Developer, I bring a strong academic background and a proven track record in the field. Armed with a Master's degree in Applied Computer Science from Dalhousie University, I have developed a deep understanding of data engineering principles and techniques. Specializing in designing and implementing ETL pipelines and optimizing database schema and SQL queries, I have consistently delivered improved query performance and enhanced data accuracy. Proficient in Airflow, DBT, and Snowflake, I have successfully processed millions of records in real-time, ensuring data integrity and quality. My expertise extends to programming languages such as Python, Java, and SQL, as well as cloud technologies like AWS and Google Cloud. Committed to agile development methodologies and armed with strong problem-solving skills, I am dedicated to delivering high-quality solutions that align with business needs."
}
